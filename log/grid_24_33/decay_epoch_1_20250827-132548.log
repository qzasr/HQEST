time_slot=15, torch_seed=42, num_link=223, num_his=8, num_pred=8, L=2, K=8, d=8, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, batch_size=8, max_epoch=100, patience=10, learning_rate=0.009, decay_epoch=1, bn_decay=0.1, drop=0.01, traffic_file='./data/Traffic_speed_(24, 33).csv', query_file='./data/query_beijing_0201_grid_start_destination_count.npz', SE_file='./data/Gf_grid_24_33_id.txt', adj_file='./data/edge_list_grid_(24,33)_weight.txt', row='24', col='33', model_file='./save_model/grid_24_33/Model_GF_test.pkl', log_file='./log/grid_24_33/log_test', SE_FC_bn_decay=0.1, SE_FC_act1='relu', SE_FC_act2='none', SE_FC_drop=0.01, TE_FC_bn_decay=0.1, TE_FC_act1='relu', TE_FC_act2='none', TE_FC_drop=0.01, FFT_bn_decay=0.1, FFT_FC_act1='relu', FFT_FC_act2='none', FFT_FC_drop=0.01, SA_Q_bn_decay=0.1, SA_Q_act='relu', SA_Q_drop=0.01, SA_K_bn_decay=0.1, SA_K_act='relu', SA_K_drop=0.01, SA_V_bn_decay=0.1, SA_V_act='relu', SA_V_drop=0.01, SA_FC_bn_decay=0.1, SA_FC_act='relu', SA_FC_drop=0.01, GMF_bn_decay=0.1, AUX_aux_bn_decay=0.1, AUX_aux_act1='relu', AUX_aux_act2='none', AUX_aux_drop=0.01, AUX_main_bn_decay=0.1, AUX_main_act1='relu', AUX_main_act2='none', AUX_main_drop=0.01, AUX_sim_bn_decay=0.1, AUX_sim_act1='relu', AUX_sim_act2='sigmoid', AUX_sim_drop=0.01, AUX_attn_bn_decay=0.1, AUX_attn_act1='relu', AUX_attn_act2='none', AUX_attn_drop=0.01, AUX_fusion_bn_decay=0.1, AUX_fusion_act1='relu', AUX_fusion_act2='none', AUX_fusion_drop=0.01, AUX_bn_decay=0.1, TA_Q_bn_decay=0.1, TA_Q_act='relu', TA_Q_drop=0.01, TA_K_bn_decay=0.1, TA_K_act='relu', TA_K_drop=0.01, TA_V_bn_decay=0.1, TA_V_act='relu', TA_V_drop=0.01, TA_fusion_bn_decay=0.1, TA_fusion_act='relu', TA_fusion_drop=0.01, NAV_Q_bn_decay=0.1, NAV_Q_act='relu', NAV_Q_drop=0.01, NAV_K_bn_decay=0.1, NAV_K_act='relu', NAV_K_drop=0.01, NAV_V_bn_decay=0.1, NAV_V_act='relu', NAV_V_drop=0.01, NAV_MAP_bn_decay=0.1, NAV_MAP_act='relu', NAV_MAP_drop=0.01, NAV_FUSION_bn_decay=0.1, NAV_FUSION_act='relu', NAV_FUSION_drop=0.01, STB_spatial_fusion_bn_decay=0.1, STB_spatial_fusion_act='relu', STB_spatial_fusion_drop=0.01, STB_temporal_fusion_bn_decay=0.1, STB_temporal_fusion_act='relu', STB_temporal_fusion_drop=0.01, STB_final_fusion_bn_decay=0.1, STB_final_fusion_act='relu', STB_final_fusion_drop=0.01, STB_gate_bn_decay=0.1, STB_gate_act1='relu', STB_gate_act2='sigmoid', STB_gate_drop=0.01, TA_transform_Q_bn_decay=0.1, TA_transform_Q_act='relu', TA_transform_Q_drop=0.0, TA_transform_K_bn_decay=0.1, TA_transform_K_act='relu', TA_transform_K_drop=0.0, TA_transform_V_bn_decay=0.1, TA_transform_V_act='relu', TA_transform_V_drop=0.0, TA_transform_FC_bn_decay=0.1, TA_transform_FC_act='relu', TA_transform_FC_drop=0.0, Traffic_Linear_bn_decay=0.1, Traffic_Linear_FC_act1='relu', Traffic_Linear_FC_act2='none', Traffic_Linear_FC_drop=0.01, Query_Linear_bn_decay=0.1, Query_Linear_FC_act1='relu', Query_Linear_FC_act2='none', Query_Linear_FC_drop=0.01, Output_bn_decay=0.1, Output_FC_act1='relu', Output_FC_act2='none', Output_FC_drop=0.01
Using device: cuda:0
trainX: torch.Size([4084, 8, 223])		 trainY: torch.Size([4084, 8, 223])
valX:   torch.Size([571, 8, 223])		valY:   torch.Size([571, 8, 223])
testX:   torch.Size([1156, 8, 223])		testY:   torch.Size([1156, 8, 223])
mean:   31.8979		std:   10.7164
data loaded!
compiling model...
fourier_filter: 9 parameters
  ├─ distribution_correction: 8 parameters
traffic_linear: 4,544 parameters
  ├─ convs: 4,544 parameters
query_linear: 4,608 parameters
  ├─ convs: 4,608 parameters
nav_attention: 25,536 parameters
  ├─ W_q: 4,288 parameters
  ├─ W_k: 4,288 parameters
  ├─ W_v: 4,288 parameters
  ├─ mapping: 4,288 parameters
  ├─ fusion: 8,384 parameters
st_embedding: 21,824 parameters
  ├─ FC_se: 8,576 parameters
  ├─ FC_te: 13,248 parameters
encoder_blocks: 302,148 parameters
  ├─ 0: 151,074 parameters
  ├─ 1: 151,074 parameters
transform_attn: 17,152 parameters
  ├─ FC_q: 4,288 parameters
  ├─ FC_k: 4,288 parameters
  ├─ FC_v: 4,288 parameters
  ├─ FC: 4,288 parameters
decoder_blocks: 302,148 parameters
  ├─ 0: 151,074 parameters
  ├─ 1: 151,074 parameters
output_layer: 4,355 parameters
  ├─ convs: 4,355 parameters
trainable parameters: 682,324
**** training model ****
2025-08-27 13:27:51 | epoch: 0001/100, training time: 91.2s, inference time: 3.2s
train loss: 2.8553, val_loss: 2.3547
val loss decrease from inf to 2.3547, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:29:28 | epoch: 0002/100, training time: 93.9s, inference time: 3.2s
train loss: 2.5319, val_loss: 2.1370
val loss decrease from 2.3547 to 2.1370, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:31:03 | epoch: 0003/100, training time: 92.7s, inference time: 3.1s
train loss: 2.4425, val_loss: 2.1289
val loss decrease from 2.1370 to 2.1289, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:32:40 | epoch: 0004/100, training time: 93.1s, inference time: 3.2s
train loss: 2.4001, val_loss: 2.1426
2025-08-27 13:34:15 | epoch: 0005/100, training time: 91.6s, inference time: 3.8s
train loss: 2.3896, val_loss: 2.1063
val loss decrease from 2.1289 to 2.1063, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:35:51 | epoch: 0006/100, training time: 92.3s, inference time: 3.7s
train loss: 2.3709, val_loss: 2.0948
val loss decrease from 2.1063 to 2.0948, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:37:25 | epoch: 0007/100, training time: 90.8s, inference time: 3.0s
train loss: 2.3553, val_loss: 2.0840
val loss decrease from 2.0948 to 2.0840, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:39:00 | epoch: 0008/100, training time: 92.3s, inference time: 3.1s
train loss: 2.3487, val_loss: 2.1008
2025-08-27 13:40:37 | epoch: 0009/100, training time: 92.9s, inference time: 3.8s
train loss: 2.3334, val_loss: 2.1701
2025-08-27 13:42:14 | epoch: 0010/100, training time: 94.2s, inference time: 3.1s
train loss: 2.3309, val_loss: 2.0932
2025-08-27 13:43:47 | epoch: 0011/100, training time: 89.6s, inference time: 3.1s
train loss: 2.3063, val_loss: 2.1103
2025-08-27 13:45:20 | epoch: 0012/100, training time: 90.1s, inference time: 3.1s
train loss: 2.2967, val_loss: 2.0580
val loss decrease from 2.0840 to 2.0580, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:46:53 | epoch: 0013/100, training time: 90.1s, inference time: 3.2s
train loss: 2.2780, val_loss: 2.0502
val loss decrease from 2.0580 to 2.0502, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:48:26 | epoch: 0014/100, training time: 89.3s, inference time: 3.1s
train loss: 2.2634, val_loss: 2.0550
2025-08-27 13:49:56 | epoch: 0015/100, training time: 87.0s, inference time: 3.1s
train loss: 2.2529, val_loss: 2.0516
2025-08-27 13:51:27 | epoch: 0016/100, training time: 88.3s, inference time: 3.4s
train loss: 2.2623, val_loss: 2.0514
2025-08-27 13:53:01 | epoch: 0017/100, training time: 90.1s, inference time: 3.7s
train loss: 2.2550, val_loss: 2.0593
2025-08-27 13:54:35 | epoch: 0018/100, training time: 90.9s, inference time: 3.1s
train loss: 2.2466, val_loss: 2.0310
val loss decrease from 2.0502 to 2.0310, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 13:56:08 | epoch: 0019/100, training time: 89.3s, inference time: 3.1s
train loss: 2.1952, val_loss: 2.0434
2025-08-27 13:57:39 | epoch: 0020/100, training time: 88.5s, inference time: 3.1s
train loss: 2.2425, val_loss: 2.0409
2025-08-27 13:59:11 | epoch: 0021/100, training time: 88.5s, inference time: 3.1s
train loss: 2.2129, val_loss: 2.0771
2025-08-27 14:00:42 | epoch: 0022/100, training time: 87.8s, inference time: 3.1s
train loss: 2.2300, val_loss: 2.0438
2025-08-27 14:02:11 | epoch: 0023/100, training time: 86.0s, inference time: 3.1s
train loss: 2.1914, val_loss: 2.0884
2025-08-27 14:03:41 | epoch: 0024/100, training time: 87.5s, inference time: 3.1s
train loss: 2.2185, val_loss: 2.0552
2025-08-27 14:05:12 | epoch: 0025/100, training time: 87.5s, inference time: 3.1s
train loss: 2.1675, val_loss: 2.0290
val loss decrease from 2.0310 to 2.0290, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-27 14:06:43 | epoch: 0026/100, training time: 87.6s, inference time: 3.1s
train loss: 2.1558, val_loss: 2.0479
2025-08-27 14:08:14 | epoch: 0027/100, training time: 88.0s, inference time: 3.2s
train loss: 2.1870, val_loss: 2.0468
2025-08-27 14:09:46 | epoch: 0028/100, training time: 89.0s, inference time: 3.2s
train loss: 2.1920, val_loss: 2.0445
2025-08-27 14:11:17 | epoch: 0029/100, training time: 87.8s, inference time: 3.1s
train loss: 2.1739, val_loss: 2.0677
2025-08-27 14:12:48 | epoch: 0030/100, training time: 88.2s, inference time: 3.2s
train loss: 2.1741, val_loss: 2.0571
2025-08-27 14:14:20 | epoch: 0031/100, training time: 88.4s, inference time: 3.1s
train loss: 2.1703, val_loss: 2.0321
2025-08-27 14:15:52 | epoch: 0032/100, training time: 89.0s, inference time: 3.1s
train loss: 2.1860, val_loss: 2.0575
2025-08-27 14:17:24 | epoch: 0033/100, training time: 88.4s, inference time: 3.2s
train loss: 2.1496, val_loss: 2.1184
2025-08-27 14:18:58 | epoch: 0034/100, training time: 91.8s, inference time: 3.1s
train loss: 2.1336, val_loss: 2.0670
2025-08-27 14:20:30 | epoch: 0035/100, training time: 88.2s, inference time: 3.1s
train loss: 2.1265, val_loss: 2.0817
early stop at epoch: 0035
Training completed. Best model saved to ./save_model/grid_24_33/Model_GF_test.pkl
**** testing model ****
loading model from ./save_model/grid_24_33/Model_GF_test.pkl
model loaded!
evaluating...
train MAE: 1.7845, RMSE: 2.7193, MAPE: 6.17%
performance in each prediction step (train)
step 1: MAE=1.5530, RMSE=2.3020, MAPE=5.36%
step 2: MAE=1.8142, RMSE=2.7545, MAPE=6.26%
step 3: MAE=1.8184, RMSE=2.7743, MAPE=6.28%
step 4: MAE=1.8053, RMSE=2.7585, MAPE=6.24%
step 5: MAE=1.8026, RMSE=2.7593, MAPE=6.23%
step 6: MAE=1.8067, RMSE=2.7693, MAPE=6.25%
step 7: MAE=1.8222, RMSE=2.7933, MAPE=6.31%
step 8: MAE=1.8539, RMSE=2.8431, MAPE=6.43%
average: MAE=1.7845, RMSE=2.7193, MAPE=6.17%
val MAE: 2.0822, RMSE: 3.1810, MAPE: 7.20%
performance in each prediction step (val)
step 1: MAE=1.7093, RMSE=2.5376, MAPE=5.91%
step 2: MAE=2.0641, RMSE=3.1440, MAPE=7.16%
step 3: MAE=2.1228, RMSE=3.2613, MAPE=7.36%
step 4: MAE=2.1397, RMSE=3.2928, MAPE=7.40%
step 5: MAE=2.1481, RMSE=3.3048, MAPE=7.43%
step 6: MAE=2.1549, RMSE=3.3056, MAPE=7.45%
step 7: MAE=2.1591, RMSE=3.3035, MAPE=7.46%
step 8: MAE=2.1599, RMSE=3.2984, MAPE=7.46%
average: MAE=2.0822, RMSE=3.1810, MAPE=7.20%
test MAE: 2.1368, RMSE: 3.2539, MAPE: 7.36%
performance in each prediction step (test)
step 1: MAE=1.7258, RMSE=2.5314, MAPE=5.97%
step 2: MAE=2.1036, RMSE=3.1830, MAPE=7.27%
step 3: MAE=2.1715, RMSE=3.3109, MAPE=7.48%
step 4: MAE=2.1958, RMSE=3.3559, MAPE=7.56%
step 5: MAE=2.2119, RMSE=3.3879, MAPE=7.61%
step 6: MAE=2.2223, RMSE=3.4091, MAPE=7.64%
step 7: MAE=2.2287, RMSE=3.4223, MAPE=7.66%
step 8: MAE=2.2348, RMSE=3.4309, MAPE=7.69%
average: MAE=2.1368, RMSE=3.2539, MAPE=7.36%
total testing time: 31.3s
total time: 55.2min
