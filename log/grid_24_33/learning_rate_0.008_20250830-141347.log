time_slot=15, torch_seed=42, num_link=223, num_his=8, num_pred=8, L=2, K=8, d=8, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, batch_size=8, max_epoch=100, patience=10, learning_rate=0.0008, decay_epoch=16, bn_decay=0.1, drop=0.01, traffic_file='./data/Traffic_speed_(24, 33).csv', query_file='./data/query_beijing_0201_grid_start_destination_count.npz', SE_file='./data/Gf_grid_24_33_id.txt', adj_file='./data/edge_list_grid_(24,33)_weight.txt', row='24', col='33', model_file='./save_model/grid_24_33/Model_GF_test.pkl', log_file='./log/grid_24_33/log_test', SE_FC_bn_decay=0.1, SE_FC_act1='relu', SE_FC_act2='none', SE_FC_drop=0.01, TE_FC_bn_decay=0.1, TE_FC_act1='relu', TE_FC_act2='none', TE_FC_drop=0.01, FFT_bn_decay=0.1, FFT_FC_act1='relu', FFT_FC_act2='none', FFT_FC_drop=0.01, SA_Q_bn_decay=0.1, SA_Q_act='relu', SA_Q_drop=0.01, SA_K_bn_decay=0.1, SA_K_act='relu', SA_K_drop=0.01, SA_V_bn_decay=0.1, SA_V_act='relu', SA_V_drop=0.01, SA_FC_bn_decay=0.1, SA_FC_act='relu', SA_FC_drop=0.01, GMF_bn_decay=0.1, AUX_aux_bn_decay=0.1, AUX_aux_act1='relu', AUX_aux_act2='none', AUX_aux_drop=0.01, AUX_main_bn_decay=0.1, AUX_main_act1='relu', AUX_main_act2='none', AUX_main_drop=0.01, AUX_sim_bn_decay=0.1, AUX_sim_act1='relu', AUX_sim_act2='sigmoid', AUX_sim_drop=0.01, AUX_attn_bn_decay=0.1, AUX_attn_act1='relu', AUX_attn_act2='none', AUX_attn_drop=0.01, AUX_fusion_bn_decay=0.1, AUX_fusion_act1='relu', AUX_fusion_act2='none', AUX_fusion_drop=0.01, AUX_bn_decay=0.1, TA_Q_bn_decay=0.1, TA_Q_act='relu', TA_Q_drop=0.01, TA_K_bn_decay=0.1, TA_K_act='relu', TA_K_drop=0.01, TA_V_bn_decay=0.1, TA_V_act='relu', TA_V_drop=0.01, TA_fusion_bn_decay=0.1, TA_fusion_act='relu', TA_fusion_drop=0.01, NAV_Q_bn_decay=0.1, NAV_Q_act='relu', NAV_Q_drop=0.01, NAV_K_bn_decay=0.1, NAV_K_act='relu', NAV_K_drop=0.01, NAV_V_bn_decay=0.1, NAV_V_act='relu', NAV_V_drop=0.01, NAV_MAP_bn_decay=0.1, NAV_MAP_act='relu', NAV_MAP_drop=0.01, NAV_FUSION_bn_decay=0.1, NAV_FUSION_act='relu', NAV_FUSION_drop=0.01, STB_spatial_fusion_bn_decay=0.1, STB_spatial_fusion_act='relu', STB_spatial_fusion_drop=0.01, STB_temporal_fusion_bn_decay=0.1, STB_temporal_fusion_act='relu', STB_temporal_fusion_drop=0.01, STB_final_fusion_bn_decay=0.1, STB_final_fusion_act='relu', STB_final_fusion_drop=0.01, STB_gate_bn_decay=0.1, STB_gate_act1='relu', STB_gate_act2='sigmoid', STB_gate_drop=0.01, TA_transform_Q_bn_decay=0.1, TA_transform_Q_act='relu', TA_transform_Q_drop=0.0, TA_transform_K_bn_decay=0.1, TA_transform_K_act='relu', TA_transform_K_drop=0.0, TA_transform_V_bn_decay=0.1, TA_transform_V_act='relu', TA_transform_V_drop=0.0, TA_transform_FC_bn_decay=0.1, TA_transform_FC_act='relu', TA_transform_FC_drop=0.0, Traffic_Linear_bn_decay=0.1, Traffic_Linear_FC_act1='relu', Traffic_Linear_FC_act2='none', Traffic_Linear_FC_drop=0.01, Query_Linear_bn_decay=0.1, Query_Linear_FC_act1='relu', Query_Linear_FC_act2='none', Query_Linear_FC_drop=0.01, Output_bn_decay=0.1, Output_FC_act1='relu', Output_FC_act2='none', Output_FC_drop=0.01
Using device: cuda:0
trainX: torch.Size([4084, 8, 223])		 trainY: torch.Size([4084, 8, 223])
valX:   torch.Size([571, 8, 223])		valY:   torch.Size([571, 8, 223])
testX:   torch.Size([1156, 8, 223])		testY:   torch.Size([1156, 8, 223])
mean:   31.8979		std:   10.7164
data loaded!
compiling model...
fourier_filter: 9 parameters
  ├─ distribution_correction: 8 parameters
traffic_linear: 4,544 parameters
  ├─ convs: 4,544 parameters
query_linear: 4,608 parameters
  ├─ convs: 4,608 parameters
nav_attention: 25,536 parameters
  ├─ W_q: 4,288 parameters
  ├─ W_k: 4,288 parameters
  ├─ W_v: 4,288 parameters
  ├─ mapping: 4,288 parameters
  ├─ fusion: 8,384 parameters
st_embedding: 21,824 parameters
  ├─ FC_se: 8,576 parameters
  ├─ FC_te: 13,248 parameters
encoder_blocks: 302,148 parameters
  ├─ 0: 151,074 parameters
  ├─ 1: 151,074 parameters
transform_attn: 17,152 parameters
  ├─ FC_q: 4,288 parameters
  ├─ FC_k: 4,288 parameters
  ├─ FC_v: 4,288 parameters
  ├─ FC: 4,288 parameters
decoder_blocks: 302,148 parameters
  ├─ 0: 151,074 parameters
  ├─ 1: 151,074 parameters
output_layer: 4,355 parameters
  ├─ convs: 4,355 parameters
trainable parameters: 682,324
**** training model ****
2025-08-30 14:16:56 | epoch: 0001/100, training time: 142.1s, inference time: 5.2s
train loss: 3.1494, val_loss: 2.6567
val loss decrease from inf to 2.6567, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:19:04 | epoch: 0002/100, training time: 124.2s, inference time: 4.1s
train loss: 2.6090, val_loss: 2.3361
val loss decrease from 2.6567 to 2.3361, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:21:19 | epoch: 0003/100, training time: 130.1s, inference time: 5.2s
train loss: 2.5030, val_loss: 2.2894
val loss decrease from 2.3361 to 2.2894, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:23:47 | epoch: 0004/100, training time: 142.4s, inference time: 5.2s
train loss: 2.4498, val_loss: 2.2144
val loss decrease from 2.2894 to 2.2144, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:26:13 | epoch: 0005/100, training time: 140.5s, inference time: 5.2s
train loss: 2.4338, val_loss: 2.1535
val loss decrease from 2.2144 to 2.1535, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:28:37 | epoch: 0006/100, training time: 138.6s, inference time: 5.2s
train loss: 2.4101, val_loss: 2.1365
val loss decrease from 2.1535 to 2.1365, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:31:04 | epoch: 0007/100, training time: 142.4s, inference time: 5.2s
train loss: 2.3945, val_loss: 2.1260
val loss decrease from 2.1365 to 2.1260, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:33:27 | epoch: 0008/100, training time: 137.7s, inference time: 5.2s
train loss: 2.3788, val_loss: 2.1646
2025-08-30 14:35:56 | epoch: 0009/100, training time: 143.6s, inference time: 5.2s
train loss: 2.3604, val_loss: 2.1440
2025-08-30 14:38:22 | epoch: 0010/100, training time: 141.2s, inference time: 5.2s
train loss: 2.3525, val_loss: 2.1019
val loss decrease from 2.1260 to 2.1019, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:40:48 | epoch: 0011/100, training time: 140.2s, inference time: 5.2s
train loss: 2.3327, val_loss: 2.1716
2025-08-30 14:43:15 | epoch: 0012/100, training time: 141.9s, inference time: 5.4s
train loss: 2.3208, val_loss: 2.0864
val loss decrease from 2.1019 to 2.0864, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:45:42 | epoch: 0013/100, training time: 141.6s, inference time: 5.1s
train loss: 2.2985, val_loss: 2.1037
2025-08-30 14:48:09 | epoch: 0014/100, training time: 141.8s, inference time: 5.3s
train loss: 2.2845, val_loss: 2.0763
val loss decrease from 2.0864 to 2.0763, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:50:32 | epoch: 0015/100, training time: 138.1s, inference time: 5.2s
train loss: 2.2803, val_loss: 2.0602
val loss decrease from 2.0763 to 2.0602, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 14:52:59 | epoch: 0016/100, training time: 141.9s, inference time: 5.2s
train loss: 2.2850, val_loss: 2.1055
2025-08-30 14:55:25 | epoch: 0017/100, training time: 140.8s, inference time: 5.2s
train loss: 2.2792, val_loss: 2.0713
2025-08-30 14:57:53 | epoch: 0018/100, training time: 142.5s, inference time: 5.2s
train loss: 2.2716, val_loss: 2.0487
val loss decrease from 2.0602 to 2.0487, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 15:00:19 | epoch: 0019/100, training time: 141.2s, inference time: 5.1s
train loss: 2.2235, val_loss: 2.0953
2025-08-30 15:02:46 | epoch: 0020/100, training time: 141.7s, inference time: 5.4s
train loss: 2.2702, val_loss: 2.0475
val loss decrease from 2.0487 to 2.0475, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 15:05:13 | epoch: 0021/100, training time: 141.1s, inference time: 5.1s
train loss: 2.2456, val_loss: 2.0831
2025-08-30 15:07:39 | epoch: 0022/100, training time: 141.0s, inference time: 5.2s
train loss: 2.2590, val_loss: 2.0715
2025-08-30 15:10:05 | epoch: 0023/100, training time: 141.0s, inference time: 5.1s
train loss: 2.2265, val_loss: 2.1046
2025-08-30 15:12:31 | epoch: 0024/100, training time: 140.9s, inference time: 5.1s
train loss: 2.2548, val_loss: 2.0763
2025-08-30 15:14:58 | epoch: 0025/100, training time: 141.5s, inference time: 5.2s
train loss: 2.2052, val_loss: 2.0421
val loss decrease from 2.0475 to 2.0421, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-30 15:17:17 | epoch: 0026/100, training time: 134.1s, inference time: 5.2s
train loss: 2.1930, val_loss: 2.0853
2025-08-30 15:19:43 | epoch: 0027/100, training time: 141.2s, inference time: 5.1s
train loss: 2.2293, val_loss: 2.1438
2025-08-30 15:22:11 | epoch: 0028/100, training time: 142.3s, inference time: 5.2s
train loss: 2.2284, val_loss: 2.0892
2025-08-30 15:24:37 | epoch: 0029/100, training time: 141.3s, inference time: 5.2s
train loss: 2.2167, val_loss: 2.0639
2025-08-30 15:27:04 | epoch: 0030/100, training time: 141.6s, inference time: 5.4s
train loss: 2.2167, val_loss: 2.0727
2025-08-30 15:29:31 | epoch: 0031/100, training time: 141.7s, inference time: 5.3s
train loss: 2.2158, val_loss: 2.0457
2025-08-30 15:32:00 | epoch: 0032/100, training time: 143.9s, inference time: 5.3s
train loss: 2.2284, val_loss: 2.1012
2025-08-30 15:34:23 | epoch: 0033/100, training time: 137.0s, inference time: 5.4s
train loss: 2.1898, val_loss: 2.1297
2025-08-30 15:36:12 | epoch: 0034/100, training time: 105.2s, inference time: 3.5s
train loss: 2.1760, val_loss: 2.0881
2025-08-30 15:37:53 | epoch: 0035/100, training time: 97.6s, inference time: 3.5s
train loss: 2.1703, val_loss: 2.1248
early stop at epoch: 0035
Training completed. Best model saved to ./save_model/grid_24_33/Model_GF_test.pkl
**** testing model ****
loading model from ./save_model/grid_24_33/Model_GF_test.pkl
model loaded!
evaluating...
train MAE: 1.8617, RMSE: 2.8122, MAPE: 6.48%
performance in each prediction step (train)
step 1: MAE=1.6535, RMSE=2.4123, MAPE=5.77%
step 2: MAE=1.8904, RMSE=2.8402, MAPE=6.58%
step 3: MAE=1.8908, RMSE=2.8611, MAPE=6.58%
step 4: MAE=1.8793, RMSE=2.8517, MAPE=6.54%
step 5: MAE=1.8775, RMSE=2.8515, MAPE=6.53%
step 6: MAE=1.8815, RMSE=2.8616, MAPE=6.55%
step 7: MAE=1.8960, RMSE=2.8858, MAPE=6.60%
step 8: MAE=1.9246, RMSE=2.9333, MAPE=6.70%
average: MAE=1.8617, RMSE=2.8122, MAPE=6.48%
val MAE: 2.1237, RMSE: 3.2242, MAPE: 7.40%
performance in each prediction step (val)
step 1: MAE=1.7717, RMSE=2.5715, MAPE=6.20%
step 2: MAE=2.1062, RMSE=3.1645, MAPE=7.36%
step 3: MAE=2.1541, RMSE=3.2787, MAPE=7.52%
step 4: MAE=2.1700, RMSE=3.3160, MAPE=7.57%
step 5: MAE=2.1856, RMSE=3.3406, MAPE=7.61%
step 6: MAE=2.1931, RMSE=3.3565, MAPE=7.62%
step 7: MAE=2.1990, RMSE=3.3739, MAPE=7.63%
step 8: MAE=2.2101, RMSE=3.3920, MAPE=7.67%
average: MAE=2.1237, RMSE=3.2242, MAPE=7.40%
test MAE: 2.1577, RMSE: 3.2691, MAPE: 7.51%
performance in each prediction step (test)
step 1: MAE=1.7864, RMSE=2.5899, MAPE=6.27%
step 2: MAE=2.1252, RMSE=3.1962, MAPE=7.41%
step 3: MAE=2.1866, RMSE=3.3237, MAPE=7.61%
step 4: MAE=2.2086, RMSE=3.3683, MAPE=7.68%
step 5: MAE=2.2239, RMSE=3.3949, MAPE=7.73%
step 6: MAE=2.2357, RMSE=3.4141, MAPE=7.76%
step 7: MAE=2.2439, RMSE=3.4268, MAPE=7.78%
step 8: MAE=2.2514, RMSE=3.4387, MAPE=7.81%
average: MAE=2.1577, RMSE=3.2691, MAPE=7.51%
total testing time: 32.5s
total time: 84.5min
